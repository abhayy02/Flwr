{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8284930,"sourceType":"datasetVersion","datasetId":4920800}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install -q flwr[simulation] ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T21:49:45.101905Z","iopub.execute_input":"2024-05-06T21:49:45.102253Z","iopub.status.idle":"2024-05-06T21:50:13.145618Z","shell.execute_reply.started":"2024-05-06T21:49:45.102225Z","shell.execute_reply":"2024-05-06T21:50:13.144493Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires protobuf<4,>3.12.2, but you have protobuf 4.25.3 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires protobuf<4.0.0dev,>=3.12.0, but you have protobuf 4.25.3 which is incompatible.\ngoogle-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.25.3 which is incompatible.\ngoogle-cloud-vision 2.8.0 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.25.3 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.3 which is incompatible.\nkfp-pipeline-spec 0.2.2 requires protobuf<4,>=3.13.0, but you have protobuf 4.25.3 which is incompatible.\npyopenssl 23.3.0 requires cryptography<42,>=41.0.5, but you have cryptography 42.0.7 which is incompatible.\ntensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.3 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\ntensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.3 which is incompatible.\ntensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.25.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.6.4 requires pydantic>=2, but you have pydantic 1.10.15 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport IPython.display as display_output\nfrom IPython.display import display, HTML, clear_output\nfrom ipywidgets import Button, Layout\nimport ipywidgets as widgets\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import random_split, DataLoader\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict\nfrom typing import List, Tuple\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport flwr as fl\nglobal classes, trainloaders, valloaders, testloader, model, num_clients, num_c","metadata":{"execution":{"iopub.status.busy":"2024-05-06T21:50:16.274600Z","iopub.execute_input":"2024-05-06T21:50:16.275466Z","iopub.status.idle":"2024-05-06T21:50:31.730897Z","shell.execute_reply.started":"2024-05-06T21:50:16.275429Z","shell.execute_reply":"2024-05-06T21:50:31.729877Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-06 21:50:23.131564: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-06 21:50:23.131663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-06 21:50:23.237946: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-06 21:50:31,716\tINFO util.py:129 -- Outdated packages:\n  ipywidgets==7.7.1 found, needs ipywidgets>=8\nRun `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Set the paths to your dataset and the train/test folders\ndata_dir = \"/kaggle/input/covid-19/Covid19\"\ntrain_dir = \"/kaggle/working/train\"\ntest_dir = \"/kaggle/working/test\"\n\n# Create the train and test folders if they don't exist\nos.makedirs(train_dir, exist_ok=True)\nos.makedirs(test_dir, exist_ok=True)\n\n# Create subfolders for the classes (Normal and Covid-19)\nos.makedirs(os.path.join(train_dir, \"Normal\"), exist_ok=True)\nos.makedirs(os.path.join(train_dir, \"COVID\"), exist_ok=True)\nos.makedirs(os.path.join(test_dir, \"Normal\"), exist_ok=True)\nos.makedirs(os.path.join(test_dir, \"COVID\"), exist_ok=True)\n\n# Set the train/test split ratio (e.g., 0.8 for 80% train, 0.2 for 20% test)\ntrain_ratio = 0.8\n\n# Loop through the classes\nfor class_name in [\"Normal\", \"COVID\"]:\n    class_dir = os.path.join(data_dir, class_name)\n    files = os.listdir(class_dir)\n    \n    # Shuffle the files randomly\n    random.shuffle(files)\n    \n    # Calculate the number of files for train and test sets\n    num_train = int(len(files) * train_ratio)\n    \n    # Copy the files to the train and test folders\n    for i, file in enumerate(files):\n        src = os.path.join(class_dir, file)\n        if i < num_train:\n            dst = os.path.join(train_dir, class_name, file)\n        else:\n            dst = os.path.join(test_dir, class_name, file)\n        shutil.copy(src, dst)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T21:50:31.732504Z","iopub.execute_input":"2024-05-06T21:50:31.732787Z","iopub.status.idle":"2024-05-06T21:51:22.607670Z","shell.execute_reply.started":"2024-05-06T21:50:31.732762Z","shell.execute_reply":"2024-05-06T21:51:22.606685Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"size_label = widgets.Label()\n\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # Required for Kaggle to work with PyTorch\nDEVICE = torch.device(\"cuda\")\ntorch.cuda.is_available()  # Check if GPU is available\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\n    f\"Training on {DEVICE} using PyTorch {torch.__version__}\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T21:51:33.037163Z","iopub.execute_input":"2024-05-06T21:51:33.037976Z","iopub.status.idle":"2024-05-06T21:51:33.095155Z","shell.execute_reply.started":"2024-05-06T21:51:33.037940Z","shell.execute_reply":"2024-05-06T21:51:33.094227Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Training on cuda using PyTorch 2.1.2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/ipywidgets/widgets/widget.py:503: DeprecationWarning: The `ipykernel.comm.Comm` class has been deprecated. Please use the `comm` module instead.For creating comms, use the function `from comm import create_comm`.\n  self.comm = Comm(**args)\n","output_type":"stream"}]},{"cell_type":"code","source":"global num_c, trainloaders, valloaders, testloader, model, classes\nnum_clients = 100\n# Dataset paths\ndataset_paths = '/kaggle/working/'\n\ndatatrain = f'{dataset_paths}/train'\ndatatset = f'{dataset_paths}/test'\nBATCH_SIZE = 32\nimg_size = (224, 224)\n\n# Download and transform \ntransform = transforms.Compose([\n        transforms.Grayscale(),\n        transforms.Resize(img_size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))\n    ])\n\ntrainset = ImageFolder(datatrain, transform=transform)\ntestset = ImageFolder(datatset, transform=transform)\nsize_label.value = f'The size of data: {len(trainset) + len(testset)}'\nprint(size_label)\n# Get the classes from the trainset\nclasses = trainset.classes\nnum_c = len(classes)\n\n# Split training set into num_clients partitions to simulate the individual dataset\npartition_size = len(trainset) // num_clients\nremainder = len(trainset) % num_clients\nlengths = [partition_size + remainder] + [partition_size] * (num_clients - 1)\ndatasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n\n# Split each partition into train/val and create DataLoader\ntrainloaders = []\nvalloaders = []\nfor ds in datasets:\n    len_val = len(ds) // 10  # 10 % validation set\n    len_train = len(ds) - len_val\n    lengths = [len_train, len_val]\n    ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n    trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n    print(\"hi\")\n    valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\ntestloader = DataLoader(testset, batch_size=BATCH_SIZE)\n\nclass Net(nn.Module):\n    def __init__(self, num_c):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n        self.pool1 = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool2 = nn.MaxPool2d(2, 2)\n        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n        self.pool3 = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.5)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(128 * 28 * 28, 264)\n        self.fc2 = nn.Linear(264,num_c)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = nn.ReLU()(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = nn.ReLU()(x)\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = nn.ReLU()(x)\n        x = self.pool3(x)\n        x = self.dropout(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = nn.ReLU()(x)\n        x = self.fc2(x)\n        return x\n\n\n# Function to train the network\ndef train(net, trainloader,valloader, epochs: int,patience: int = 5, verbose=True):\n    \"\"\"Train the network on the training set.\"\"\"\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters())\n    net.train()\n    best_val_acc = 0.0\n    early_stop_counter = 0\n    for epoch in range(epochs):\n        correct, total, epoch_loss = 0, 0, 0.0\n        for images, labels in trainloader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            optimizer.zero_grad()\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            # Metrics\n            epoch_loss += loss\n            total += labels.size(0)\n            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n        epoch_loss /= len(trainloader.dataset)\n        epoch_acc = correct / total\n        val_loss, val_acc = test(net, valloader)\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            early_stop_counter = 0\n        else:\n            early_stop_counter += 1\n            if early_stop_counter >= patience:\n                print(f\"Early stopping after {epoch + 1} epochs.\")\n                break\n\n        if verbose:\n            print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}, val loss {val_loss}, val accuracy {val_acc}\")\n\n# Function to test the network\ndef test(net, testloader):\n    \"\"\"Evaluate the network on the entire test set.\"\"\"\n    criterion = torch.nn.CrossEntropyLoss()\n    correct, total, loss = 0, 0, 0.0\n    net.eval()\n    with torch.no_grad():\n        for images, labels in testloader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = net(images)\n            loss += criterion(outputs, labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    loss /= len(testloader.dataset)\n    accuracy = correct / total\n    return loss, accuracy\n\n# Create the neural network model\nmodel = Net(num_c).to(DEVICE)\nprint(\"Number of classes:\", num_c)\nprint('num_clients', num_clients)\nprint('Waiting for the plot to display...')\n\n\n\n# Create a dictionary to store one image from each class\nclass_images = {}\n\n# Iterate over the trainloaders to get one image from each class\nfor trainloader in trainloaders:\n    for images, labels in trainloader:\n        images = images.to(DEVICE)\n        labels = labels.to(DEVICE)\n        for image, label in zip(images, labels):\n            class_name = classes[label]\n            if class_name not in class_images:\n                # Reshape and convert the image to a NumPy array\n                image = image.permute(1, 2, 0).cpu().numpy()\n                # Denormalize\n                image = image / 2 + 0.5\n                class_images[class_name] = image\n                break\n\n# Create a figure and a grid of subplots\nfig, axs = plt.subplots(1, len(class_images), figsize=(5, 3))\n\n# Loop over the classes and plot the corresponding image\nfor i, (class_name, image) in enumerate(class_images.items()):\n    ax = axs[i]\n    ax.imshow(image)\n    ax.set_title(class_name)\n    ax.axis(\"off\")\n\n# Set the title of the plot\nfig.suptitle('Sample from selected classes', fontsize=14)\n\n# Show the plot\nplt.tight_layout()\nplt.rcParams['figure.dpi'] = 100\n\n# Create an output widget\noutput_widget = widgets.Output()\n\n# Display the plot inside the output widget\nwith output_widget:\n    plt.show()\n\n# Display the output widget\ndisplay_output.display(output_widget)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T21:51:38.621221Z","iopub.execute_input":"2024-05-06T21:51:38.621820Z","iopub.status.idle":"2024-05-06T21:52:09.346446Z","shell.execute_reply.started":"2024-05-06T21:51:38.621789Z","shell.execute_reply":"2024-05-06T21:52:09.345518Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Label(value='The size of data: 13808')\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nhi\nNumber of classes: 2\nnum_clients 100\nWaiting for the plot to display...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6359a4bc924d929ff3bb16b3e6e33c"}},"metadata":{}}]},{"cell_type":"code","source":"print(\n    f\"Training on {DEVICE} using PyTorch {torch.__version__}\"\n)\nprint(len(trainloaders))\n\ninput_num_epochs = 100\ntrainloader = trainloaders[0]  # Select the first trainloader for centralized training\nvalloader = valloaders[0]  # Select the first valloader for centralized training\naccuracy_history = []\n\n# Pass valloader as an additional argument to train\ntrain(model.to(DEVICE), trainloader, valloader, input_num_epochs)\nloss, accuracy = test(model.to(DEVICE), testloader)\naccuracy_history.append(accuracy)\n\nmessage = 'training is over'\ndisplay(HTML(message))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T21:52:34.165593Z","iopub.execute_input":"2024-05-06T21:52:34.165957Z","iopub.status.idle":"2024-05-06T21:52:55.224187Z","shell.execute_reply.started":"2024-05-06T21:52:34.165929Z","shell.execute_reply":"2024-05-06T21:52:55.223309Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Training on cuda using PyTorch 2.1.2\n100\nEpoch 1: train loss 0.03319573402404785, accuracy 0.4642857142857143, val loss 0.0464765469233195, val accuracy 0.2\nEpoch 2: train loss 0.023863280192017555, accuracy 0.6214285714285714, val loss 0.03942335446675618, val accuracy 0.7333333333333333\nEpoch 3: train loss 0.021085131913423538, accuracy 0.7214285714285714, val loss 0.03535428444544474, val accuracy 0.7333333333333333\nEpoch 4: train loss 0.020009122788906097, accuracy 0.7214285714285714, val loss 0.03360588153203328, val accuracy 0.7333333333333333\nEpoch 5: train loss 0.01901368238031864, accuracy 0.7214285714285714, val loss 0.030705310901006064, val accuracy 0.7333333333333333\nEpoch 6: train loss 0.018143899738788605, accuracy 0.75, val loss 0.027262012163798015, val accuracy 0.7333333333333333\nEpoch 7: train loss 0.016199585050344467, accuracy 0.7571428571428571, val loss 0.024849385023117065, val accuracy 0.8\nEpoch 8: train loss 0.016044290736317635, accuracy 0.8071428571428572, val loss 0.02159579594930013, val accuracy 0.8\nEpoch 9: train loss 0.013725979253649712, accuracy 0.7857142857142857, val loss 0.02315145532290141, val accuracy 0.9333333333333333\nEpoch 10: train loss 0.012546077370643616, accuracy 0.8428571428571429, val loss 0.017695740858713786, val accuracy 0.9333333333333333\nEpoch 11: train loss 0.01122441329061985, accuracy 0.8357142857142857, val loss 0.01849718689918518, val accuracy 0.9333333333333333\nEpoch 12: train loss 0.00897340476512909, accuracy 0.8642857142857143, val loss 0.022276655832926432, val accuracy 0.9333333333333333\nEpoch 13: train loss 0.007347083184868097, accuracy 0.9357142857142857, val loss 0.014319037397702536, val accuracy 0.9333333333333333\nEarly stopping after 14 epochs.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"training is over"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}